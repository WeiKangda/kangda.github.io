---
title: "When Do Decompositions Help for Machine Reading?"
collection: publications
permalink: /publication/2015-10-01-paper-title-number-3
excerpt: '
![paper3](https://weikangda.github.io/kangda.github.io/images/paper3.PNG){: .align-left width="300px"}  

Answering complex questions often requires multi-step reasoning in order to obtain the final answer. Most research into decompositions of complex questions involves open-domain systems, which have shown success in using these decompositions for improved retrieval. In the machine reading setting, however, work to understand when decompositions are helpful is understudied. We conduct experiments on decompositions in machine reading to unify recent work in this space, using a range of models and datasets. We find that decompositions can be helpful in zero or limited-data settings, giving several points of improvement in exact match. However, we also show that when models are given access to around a few hundred or more examples, decompositions are not helpful (and can actually be detrimental). Thus, our analysis implies that models can learn decompositions implicitly even with limited data.'
date: 2023-10-06
venue: 'Proceedings of the Empirical Methods in Natural Language Processing 2023'
paperurl: 
citation:
---
![paper3](https://weikangda.github.io/kangda.github.io/images/paper3.PNG)  

Answering complex questions often requires multi-step reasoning in order to obtain the final answer. Most research into decompositions of complex questions involves open-domain systems, which have shown success in using these decompositions for improved retrieval. In the machine reading setting, however, work to understand when decompositions are helpful is understudied. We conduct experiments on decompositions in machine reading to unify recent work in this space, using a range of models and datasets. We find that decompositions can be helpful in zero or limited-data settings, giving several points of improvement in exact match. However, we also show that when models are given access to around a few hundred or more examples, decompositions are not helpful (and can actually be detrimental). Thus, our analysis implies that models can learn decompositions implicitly even with limited data.

[Download paper here](https://arxiv.org/pdf/2212.10019.pdf)